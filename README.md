# Security_toolkit_1

## Goal
Use LLM Guard to detect and reduce risks like prompt injection, secrets leakage, and harmful content.

## Repo Structure
- scripts/ : runnable demo scripts
- notebooks/ : experiments
- data/ : test prompts / small datasets
- results/ : outputs, screenshots, tables
- report/ : written report files
- docs/ : diagrams and notes

## Setup
Coming soon (will include Python environment + dependencies).
